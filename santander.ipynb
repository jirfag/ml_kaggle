{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76020, 369) (76020,)\n",
      "0    73012\n",
      "1     3008\n",
      "Name: TARGET, dtype: int64\n",
      "CPU times: user 5.83 s, sys: 1.08 s, total: 6.91 s\n",
      "Wall time: 7.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "RANDOM_STATE = 42\n",
    "import pandas as pd\n",
    "train_data = pd.read_csv('train.csv', index_col='ID')\n",
    "train_data = train_data.sample(frac=1.0)\n",
    "\n",
    "test_data = pd.read_csv('test.csv', index_col='ID')\n",
    "\n",
    "y_train = train_data['TARGET']\n",
    "train_data.drop(['TARGET'], axis=1, inplace=True)\n",
    "X_train = train_data\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(y_train.value_counts())\n",
    "assert not (X_train.count() != y_train.shape[0]).any() # ensure that there is no missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns are duplicates:  ['ind_var2_0', 'ind_var2', 'ind_var6_0', 'ind_var6', 'ind_var13_medio_0', 'ind_var18_0', 'ind_var26_0', 'ind_var25_0', 'ind_var27_0', 'ind_var28_0', 'ind_var28', 'ind_var27', 'ind_var32_0', 'ind_var34_0', 'ind_var37_0', 'ind_var40', 'ind_var41', 'ind_var46_0', 'ind_var46', 'num_var6_0', 'num_var6', 'num_var18_0', 'num_var26_0', 'num_var25_0', 'num_var27_0', 'num_var28_0', 'num_var28', 'num_var27', 'num_var32_0', 'num_var34_0', 'num_var37_0', 'num_var40', 'num_var41', 'num_var46_0', 'num_var46', 'saldo_var28', 'saldo_var27', 'saldo_var41', 'saldo_var46', 'delta_imp_reemb_var13_1y3', 'delta_imp_reemb_var17_1y3', 'delta_imp_reemb_var33_1y3', 'delta_imp_trasp_var17_in_1y3', 'delta_imp_trasp_var17_out_1y3', 'delta_imp_trasp_var33_in_1y3', 'delta_imp_trasp_var33_out_1y3', 'imp_amort_var18_hace3', 'imp_amort_var34_hace3', 'imp_reemb_var13_hace3', 'imp_reemb_var33_hace3', 'imp_trasp_var17_out_hace3', 'imp_trasp_var33_out_hace3', 'num_var2_0_ult1', 'num_var2_ult1', 'num_reemb_var13_hace3', 'num_reemb_var33_hace3', 'num_trasp_var17_out_hace3', 'num_trasp_var33_out_hace3', 'saldo_var2_ult1', 'saldo_var6']\n",
      "categorical columns count:  146\n",
      "data transformation: 369 -> 145 features\n",
      "CPU times: user 23.2 s, sys: 3.95 s, total: 27.2 s\n",
      "Wall time: 27.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.feature_selection import SelectKBest, SelectPercentile, f_classif, VarianceThreshold\n",
    "import numpy as np\n",
    "\n",
    "def remove_constant_features(X):\n",
    "    non_const_cols = (X != X.ix[1]).any()\n",
    "    return X.loc[:, non_const_cols]\n",
    "\n",
    "def get_best_features(X, y):\n",
    "    selectK = SelectPercentile(f_classif, 90)\n",
    "    selectK.fit(X[:y.shape[0]], y)\n",
    "    return pd.DataFrame(selectK.transform(X))\n",
    "\n",
    "def replace_outliers_with_mean(X):\n",
    "    for col in X.columns:\n",
    "        mean, std = X[col].mean(), X[col].std()\n",
    "        outliers = (X[col] - mean).abs() > 3 * std\n",
    "        X[outliers] = X[col][~outliers].mean()\n",
    "    return X\n",
    "\n",
    "from pandas.core.common import array_equivalent\n",
    "\n",
    "def get_duplicate_columns(frame):\n",
    "    groups = frame.columns.to_series().groupby(frame.dtypes).groups\n",
    "    dups = []\n",
    "\n",
    "    for t, v in groups.items():\n",
    "\n",
    "        cs = frame[v].columns\n",
    "        vs = frame[v]\n",
    "        lcs = len(cs)\n",
    "\n",
    "        for i in range(lcs):\n",
    "            ia = vs.iloc[:,i].values\n",
    "            for j in range(i+1, lcs):\n",
    "                ja = vs.iloc[:,j].values\n",
    "                if array_equivalent(ia, ja):\n",
    "                    dups.append(cs[i])\n",
    "                    break\n",
    "\n",
    "    return dups\n",
    "\n",
    "def remove_dup_cols(X):\n",
    "    dup_cols = get_duplicate_columns(X)\n",
    "    print('columns are duplicates: ', dup_cols)\n",
    "\n",
    "    # too high memory usage\n",
    "    #return X.T.drop_duplicates().T\n",
    "\n",
    "    return X.drop(dup_cols, axis=1)\n",
    "\n",
    "def encode_categorical_columns(X):\n",
    "    categorical_cols = []\n",
    "    for col_name in X.loc[:, X.dtypes == np.int64]:\n",
    "        col = X[col_name]\n",
    "        if col.value_counts().count() <= 10:\n",
    "#            print('vc of {}: {}'.format(col_name, col.value_counts()))\n",
    "            categorical_cols.append(col_name)\n",
    "\n",
    "    print('categorical columns count: ', len(categorical_cols))\n",
    "    if not categorical_cols:\n",
    "        return X\n",
    "    \n",
    "    dummies = pd.get_dummies(X[categorical_cols])\n",
    "    ret = pd.concat([X, dummies], axis=1)\n",
    "    ret.drop(categorical_cols, axis=1, inplace=True)\n",
    "    return ret\n",
    "    \n",
    "def data_transform(X, y):\n",
    "    X = remove_dup_cols(X)\n",
    "    X = remove_constant_features(X)\n",
    "    X = encode_categorical_columns(X)\n",
    "    X = get_best_features(X, y)\n",
    "    return X\n",
    " \n",
    "# XXX: it decreses auc on train set, don't use\n",
    "# BUT: one time it increased auc on test set for 0.003\n",
    "#    X = get_best_features(X, y)\n",
    "\n",
    "# XXX: it dramatically decreses auc on train set, don't use\n",
    "#df = pd.DataFrame(x)\n",
    "#return replace_outliers_with_mean(df)\n",
    "\n",
    "X_test = test_data\n",
    "X_all = pd.concat([X_train, X_test])\n",
    "X_all_transformed = data_transform(X_all, y_train)\n",
    "\n",
    "X_train_transformed, X_test_transformed = X_all_transformed[:X_train.shape[0]], X_all_transformed[X_train.shape[0]:]\n",
    "assert(X_train_transformed.shape[0] == X_train.shape[0])\n",
    "assert(X_test_transformed.shape[0] == X_test.shape[0])\n",
    "assert X_train_transformed.shape[1] == X_test_transformed.shape[1]\n",
    "\n",
    "print('data transformation: {} -> {} features'.format(X_train.shape[1], X_train_transformed.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-0dc81f196a35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'import xgboost as xgb\\nimport numpy as np\\n\\ndef report(grid_scores, n_top=3):\\n    import numpy as np\\n    from operator import itemgetter\\n    top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]\\n    for i, score in enumerate(top_scores):\\n        print(\"Model with rank: {0}\".format(i + 1))\\n        print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\\n              score.mean_validation_score,\\n              np.std(score.cv_validation_scores)))\\n        print(\"Parameters: {0}\".format(score.parameters))\\n        print(\"\")\\n\\ndef find_best_xgb_params(X, y, X_all, y_all, n_iter):\\n    from sklearn.grid_search import RandomizedSearchCV\\n\\n    clf = xgb.XGBClassifier(missing=np.nan, seed=RANDOM_STATE)\\n    \\n    param_distributions = {\\n        \\'max_depth\\': np.arange(3, 7),\\n        \\'n_estimators\\': [1000], # early stop helps us\\n        \\'learning_rate\\': np.random.uniform(0.01, 0.1, 30),\\n        \\'nthread\\': [4],\\n        \\'subsample\\': np.arange(0.5, 1.05, 0.05),\\n        \\'colsample_bytree\\': np.arange(0.5, 1.05, 0.05),\\n        \\'seed\\': [RANDOM_STATE],\\n    }\\n    fit_params = {\\'early_stopping_rounds\\': 30, \\'eval_metric\\': \"auc\", \\'verbose\\': False, \\'eval_set\\': [(X_all, y_all)]}\\n    rs = RandomizedSearchCV(estimator=clf, param_distributions=param_distributions,\\n                            n_iter=n_iter, n_jobs=1, refit=False,\\n                            cv=2, random_state=RANDOM_STATE,\\n                            scoring=\\'roc_auc\\', fit_params=fit_params, verbose=1)\\n\\n    rs.fit(X, y)\\n    report(rs.grid_scores_, n_top=3)\\n\\n\\n    \\ndef find_best_xgb_params_with_hyperopt(X, y, X_check, y_check, n_iter):\\n    from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\\n    from sklearn.metrics import roc_auc_score\\n    \\n    X_train, y_train = X[:int(X.shape[0]/2)], y[:int(X.shape[0]/2)]\\n    X_test, y_test = X[int(X.shape[0]/2):], y[int(X.shape[0]/2):]\\n    \\n    def objective(s):\\n        clf = xgb.XGBClassifier(missing=np.nan, n_estimators=10000,\\n                                nthread=4, seed=RANDOM_STATE,\\n                                max_depth=int(s[\\'max_depth\\']),\\n                                min_child_weight=s[\\'min_child_weight\\'],\\n                                learning_rate=s[\\'learning_rate\\'],\\n                                subsample=0.7,\\n                                colsample_bytree=0.7,\\n                               )\\n        eval_set = [(X_test, y_test)]\\n\\n        clf.fit(X_train, y_train,\\n                eval_set=eval_set, eval_metric=\"auc\", \\n                early_stopping_rounds=30)\\n\\n        pred = clf.predict_proba(X_check)[:,1]\\n        auc = roc_auc_score(y_check, pred)\\n        nonlocal iter_no\\n        print(\"#{}/{}: SCORE={}, best_tree_n={}, space={}\".format(iter_no, n_iter, auc, clf.best_iteration, s))\\n        iter_no += 1\\n        return {\\'loss\\':1-auc, \\'status\\': STATUS_OK }\\n    \\n    iter_no = 1\\n    space = {\\n        \\'max_depth\\': hp.quniform(\"max_depth\", 2, 6, 1),\\n        \\'min_child_weight\\': hp.quniform(\\'min_child\\', 1, 20, 1),\\n        \\'learning_rate\\': hp.uniform(\\'learning_rate\\', 0.01, 0.03),\\n#        \\'subsample\\': hp.uniform(\\'subsample\\', 0.7, 1),\\n#        \\'colsample_bytree\\': hp.uniform(\\'colsample_bytree\\', 0.7, 1.0),\\n        \\'scale_pos_weight\\': hp.uniform(\\'scale_pos_weight\\', 0.1, 1.0)\\n    }\\n\\n\\n    trials = Trials()\\n    best = fmin(fn=objective,\\n                space=space,\\n                algo=tpe.suggest,\\n                max_evals=n_iter,\\n                trials=trials)\\n    print(best)\\n\\ntrain_subsamples_n = 40000\\n\\n#find_best_xgb_params(x_, y_, X_train_transformed, y_train, n_iter=100)\\nfind_best_xgb_params_with_hyperopt(X_train_transformed[:train_subsamples_n], y_train[:train_subsamples_n],\\n                                   X_train_transformed[train_subsamples_n:], y_train[train_subsamples_n:],\\n                                   n_iter=400)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/opt/conda/lib/python3.4/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2291\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2292\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2293\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2294\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2295\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python3.4/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python3.4/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1165\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1166\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1167\u001b[1;33m             \u001b[0mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1168\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1169\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36mfind_best_xgb_params_with_hyperopt\u001b[1;34m(X, y, X_check, y_check, n_iter)\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python3.4/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin)\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 306\u001b[1;33m             \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m             )\n\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python3.4/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin)\u001b[0m\n\u001b[0;32m    631\u001b[0m             \u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 633\u001b[1;33m             return_argmin=return_argmin)\n\u001b[0m\u001b[0;32m    634\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python3.4/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin)\u001b[0m\n\u001b[0;32m    317\u001b[0m                     verbose=verbose)\n\u001b[0;32m    318\u001b[0m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    320\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python3.4/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python3.4/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    170\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m                 \u001b[1;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstopped\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python3.4/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'job exception: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python3.4/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    836\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[1;32m--> 838\u001b[1;33m             \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    839\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36mobjective\u001b[1;34m(s)\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python3.4/site-packages/xgboost-0.4-py3.4.egg/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose)\u001b[0m\n\u001b[0;32m    414\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 416\u001b[1;33m                               verbose_eval=verbose)\n\u001b[0m\u001b[0;32m    417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python3.4/site-packages/xgboost-0.4-py3.4.egg/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, learning_rates, xgb_model)\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[1;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m  \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[0mversion\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python3.4/site-packages/xgboost-0.4-py3.4.egg/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    748\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 749\u001b[1;33m             \u001b[0m_check_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBoosterUpdateOneIter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    750\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    751\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "def report(grid_scores, n_top=3):\n",
    "    import numpy as np\n",
    "    from operator import itemgetter\n",
    "    top_scores = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]\n",
    "    for i, score in enumerate(top_scores):\n",
    "        print(\"Model with rank: {0}\".format(i + 1))\n",
    "        print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "              score.mean_validation_score,\n",
    "              np.std(score.cv_validation_scores)))\n",
    "        print(\"Parameters: {0}\".format(score.parameters))\n",
    "        print(\"\")\n",
    "\n",
    "def find_best_xgb_params(X, y, X_all, y_all, n_iter):\n",
    "    from sklearn.grid_search import RandomizedSearchCV\n",
    "\n",
    "    clf = xgb.XGBClassifier(missing=np.nan, seed=RANDOM_STATE)\n",
    "    \n",
    "    param_distributions = {\n",
    "        'max_depth': np.arange(3, 7),\n",
    "        'n_estimators': [1000], # early stop helps us\n",
    "        'learning_rate': np.random.uniform(0.01, 0.1, 30),\n",
    "        'nthread': [4],\n",
    "        'subsample': np.arange(0.5, 1.05, 0.05),\n",
    "        'colsample_bytree': np.arange(0.5, 1.05, 0.05),\n",
    "        'seed': [RANDOM_STATE],\n",
    "    }\n",
    "    fit_params = {'early_stopping_rounds': 30, 'eval_metric': \"auc\", 'verbose': False, 'eval_set': [(X_all, y_all)]}\n",
    "    rs = RandomizedSearchCV(estimator=clf, param_distributions=param_distributions,\n",
    "                            n_iter=n_iter, n_jobs=1, refit=False,\n",
    "                            cv=2, random_state=RANDOM_STATE,\n",
    "                            scoring='roc_auc', fit_params=fit_params, verbose=1)\n",
    "\n",
    "    rs.fit(X, y)\n",
    "    report(rs.grid_scores_, n_top=3)\n",
    "\n",
    "\n",
    "    \n",
    "def find_best_xgb_params_with_hyperopt(X, y, X_check, y_check, n_iter):\n",
    "    from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    \n",
    "    X_train, y_train = X[:int(X.shape[0]/2)], y[:int(X.shape[0]/2)]\n",
    "    X_test, y_test = X[int(X.shape[0]/2):], y[int(X.shape[0]/2):]\n",
    "    \n",
    "    def objective(s):\n",
    "        clf = xgb.XGBClassifier(missing=np.nan, n_estimators=10000,\n",
    "                                nthread=4, seed=RANDOM_STATE,\n",
    "                                max_depth=int(s['max_depth']),\n",
    "                                min_child_weight=s['min_child_weight'],\n",
    "                                learning_rate=s['learning_rate'],\n",
    "                                subsample=0.7,\n",
    "                                colsample_bytree=0.7,\n",
    "                               )\n",
    "        eval_set = [(X_test, y_test)]\n",
    "\n",
    "        clf.fit(X_train, y_train,\n",
    "                eval_set=eval_set, eval_metric=\"auc\", \n",
    "                early_stopping_rounds=30)\n",
    "\n",
    "        pred = clf.predict_proba(X_check)[:,1]\n",
    "        auc = roc_auc_score(y_check, pred)\n",
    "        nonlocal iter_no\n",
    "        print(\"#{}/{}: SCORE={}, best_tree_n={}, space={}\".format(iter_no, n_iter, auc, clf.best_iteration, s))\n",
    "        iter_no += 1\n",
    "        return {'loss':1-auc, 'status': STATUS_OK }\n",
    "    \n",
    "    iter_no = 1\n",
    "    space = {\n",
    "        'max_depth': hp.quniform(\"max_depth\", 2, 6, 1),\n",
    "        'min_child_weight': hp.quniform('min_child', 1, 20, 1),\n",
    "        'learning_rate': hp.uniform('learning_rate', 0.01, 0.03),\n",
    "#        'subsample': hp.uniform('subsample', 0.7, 1),\n",
    "#        'colsample_bytree': hp.uniform('colsample_bytree', 0.7, 1.0),\n",
    "        'scale_pos_weight': hp.uniform('scale_pos_weight', 0.1, 1.0)\n",
    "    }\n",
    "\n",
    "\n",
    "    trials = Trials()\n",
    "    best = fmin(fn=objective,\n",
    "                space=space,\n",
    "                algo=tpe.suggest,\n",
    "                max_evals=n_iter,\n",
    "                trials=trials)\n",
    "    print(best)\n",
    "\n",
    "train_subsamples_n = 40000\n",
    "\n",
    "#find_best_xgb_params(x_, y_, X_train_transformed, y_train, n_iter=100)\n",
    "find_best_xgb_params_with_hyperopt(X_train_transformed[:train_subsamples_n], y_train[:train_subsamples_n],\n",
    "                                   X_train_transformed[train_subsamples_n:], y_train[train_subsamples_n:],\n",
    "                                   n_iter=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stacking xgb auc: 0.83764\n",
      "CPU times: user 53.9 s, sys: 770 ms, total: 54.7 s\n",
      "Wall time: 20.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "# also working params: missing=9999999999, max_depth = 4, n_estimators=200, learning_rate=0.001,\n",
    "# nthread=4, subsample=0.75, colsample_bytree=0.75, seed=4242\n",
    "\n",
    "#[125] cv-mean:0.8293896666666667\tcv-std:0.003718656507694988\n",
    "clf_params_from_kaggle_scripts = {'max_depth': 5, 'learning_rate': 0.03,\n",
    "                                  'subsample': 0.95, 'colsample_bytree': 0.85}\n",
    "\n",
    "#[48] cv-mean:0.8289763333333333\tcv-std:0.004993271561700667\n",
    "clf_params_from_hyperopt = {'learning_rate': 0.023533308727518153, 'colsample_bytree': 0.7028779789099826,\n",
    "                            'max_depth': 4, 'subsample': 0.7408078537264539, 'min_child_weight': 10.0}\n",
    "\n",
    "clf_params = dict(clf_params_from_hyperopt)\n",
    "clf_params['missing'] = np.nan\n",
    "clf_params['seed'] = RANDOM_STATE\n",
    "clf_params['nthread'] = 4\n",
    "clf_params['n_estimators'] = 10000\n",
    "\n",
    "def do_xgb_cv(X, y):\n",
    "    dtrain = xgb.DMatrix(X, label=y, missing=clf_params['missing'])\n",
    "    xgb.cv(clf_params, dtrain, clf_params['n_estimators'], nfold=3, metrics='auc',\n",
    "           early_stopping_rounds=20, verbose_eval=True, seed=RANDOM_STATE)\n",
    "\n",
    "def xgb_predict(X_train, y_train, X_test, n_estimators):\n",
    "    params = dict(clf_params)\n",
    "    params['n_estimators'] = n_estimators\n",
    "    xgb_clf = xgb.XGBClassifier(**params)\n",
    "    xgb_clf.fit(X_train, y_train, verbose=True)\n",
    "\n",
    "    return xgb_clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "# ipython crashes often on full collection, so run on subcollection\n",
    "cv_limit = int(X_train_transformed.shape[0]/2)\n",
    "if False:\n",
    "    do_xgb_cv(X_train_transformed[:cv_limit], y_train[:cv_limit])\n",
    "\n",
    "n_estimators = 238 # get it from best iteration of xgb.cv\n",
    "\n",
    "# double check CV score for overfitting check\n",
    "if False:\n",
    "    X_train_test, y_train_test = X_train_transformed[:cv_limit], y_train[:cv_limit]\n",
    "    y_train_test_predicted = xgb_predict(X_train_transformed[cv_limit:], y_train[cv_limit:],\n",
    "                                         X_train_test, n_estimators=n_estimators)\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    auc = roc_auc_score(y_train_test, y_train_test_predicted)\n",
    "    print('double checked AUC is ', auc)\n",
    "\n",
    "def do_stacking(X_train_, y_train_, X_predict_):\n",
    "    from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\n",
    "    #from sklearn.grid_search import GridSearchCV \n",
    "    #rf_search_params = {'n_estimators': list(range(100, 301, 100)), 'max_features': np.arange(0.8, 1.01, 0.1)}\n",
    "    \n",
    "    rf_clf = RandomForestClassifier(n_jobs=4, n_estimators=50, random_state=RANDOM_STATE)\n",
    "    et_clf = ExtraTreesClassifier(n_estimators=50, random_state=RANDOM_STATE)\n",
    "    ab_clf = AdaBoostClassifier(n_estimators=50, random_state=RANDOM_STATE)\n",
    "\n",
    "    stacking_classifiers = [rf_clf, et_clf, ab_clf]  \n",
    "    X_predict_stacked_ = X_predict_.copy()\n",
    "\n",
    "    for clf in stacking_classifiers:\n",
    "        clf.fit(X_train_, y_train_)\n",
    "        if hasattr(clf, 'predict_proba'):\n",
    "            clf_prediction = clf.predict_proba(X_predict_)[:,1]\n",
    "        else:\n",
    "            clf_prediction = clf.predict(X_predict_)\n",
    "        X_predict_stacked_[clf.__class__.__name__] = pd.Series(clf_prediction, index=X_predict_stacked_.index)\n",
    "    return X_predict_stacked_\n",
    "    \n",
    "def do_submission():\n",
    "    X_train_transformed_stacked = do_stacking(X_train_transformed, y_train, X_train_transformed)\n",
    "    y_pred = xgb_predict(X_train_transformed_stacked, y_train, X_test_transformed, n_estimators=n_estimators)\n",
    "    submission = pd.DataFrame({\"ID\": test_data.index, \"TARGET\": y_pred})\n",
    "    submission.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "#double checked AUC is  0.839140157487\n",
    "#CPU times: user 56.2 s, sys: 780 ms, total: 57 s\n",
    "#Wall time: 14.5 s\n",
    "\n",
    "\n",
    "def run_stacking_with_xgb():  \n",
    "    # train stacking classifiers on first 1/3 of training set\n",
    "    train_size = X_train_transformed.shape[0]\n",
    "    train_subset_size = int(train_size / 3)\n",
    "    X_train_transformed_stacked = do_stacking(X_train_transformed[:train_subset_size],\n",
    "                                              y_train[:train_subset_size], X_train_transformed)\n",
    "    \n",
    "    # train xgb over initial training set + stacking classifiers result on second 1/3 of training set\n",
    "    params = dict(clf_params)\n",
    "    params['n_estimators'] = n_estimators\n",
    "    xgb_clf = xgb.XGBClassifier(**params)\n",
    "    xgb_clf.fit(X_train_transformed_stacked[train_subset_size:-train_subset_size],\n",
    "                y_train[train_subset_size:-train_subset_size])\n",
    "    \n",
    "    # test stacking model on third 1/3 of training set\n",
    "    y_predicted = xgb_clf.predict_proba(X_train_transformed_stacked[-train_subset_size:])[:,1]\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    auc = roc_auc_score(y_train[-train_subset_size:], y_predicted)\n",
    "    print('stacking xgb auc: {:.5f}'.format(auc))\n",
    "\n",
    "if True:\n",
    "    run_stacking_with_xgb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgb: 0.83588 for 29.8 sec\n",
      "gbt: 0.83175 for 29.0 sec\n",
      "random_forest: 0.75196 for 32.4 sec\n",
      "extra_trees: 0.69849 for 38.8 sec\n",
      "ada_boost: 0.81779 for 4.5 sec\n",
      "lin_reg: 0.78822 for 4.7 sec\n",
      "dec_tree: 0.57324 for 6.8 sec\n",
      "svm: 0.29054 for 2.6 sec\n",
      "knn: 0.52334 for 13.8 sec\n",
      "sgd: 0.62868 for 2.1 sec\n",
      "bayes_ridge: 0.77733 for 13.1 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.4/site-packages/sklearn/linear_model/coordinate_descent.py:474: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations\n",
      "  ConvergenceWarning)\n",
      "/opt/conda/lib/python3.4/site-packages/sklearn/linear_model/coordinate_descent.py:474: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elastic_net: 0.69058 for 48.0 sec\n",
      "log_reg: 0.60418 for 6.3 sec\n",
      "perceptron: 0.62868 for 2.1 sec\n"
     ]
    }
   ],
   "source": [
    "def compare_estimators():\n",
    "    import time\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    from sklearn import svm\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.linear_model import SGDClassifier, BayesianRidge, ElasticNet, LinearRegression, LogisticRegression, Perceptron\n",
    "    from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    import xgboost as xgb\n",
    "\n",
    "    estimators = [\n",
    "                  {'name': 'xgb', 'clf': xgb.XGBClassifier, 'params': {'no_random_state': True}, 'clf_params': {'n_estimators': 40, 'learning_rate': 0.2}},\n",
    "                  {'name': 'gbt', 'clf': GradientBoostingClassifier, 'clf_params': {'n_estimators': 40, 'learning_rate': 0.2}},\n",
    "                  {'name': 'random_forest', 'clf': RandomForestClassifier, 'clf_params': {'n_estimators': 100, 'n_jobs': 4}},\n",
    "                  {'name': 'extra_trees', 'clf': ExtraTreesClassifier, 'clf_params': {'n_estimators': 100, 'n_jobs': 2}},\n",
    "                  {'name': 'ada_boost', 'clf': AdaBoostClassifier,\n",
    "                   'clf_params': {'n_estimators': 10}},\n",
    "                  {'name': 'lin_reg', 'clf': LinearRegression, 'params': {'no_random_state': True}},\n",
    "\n",
    "                  {'name': 'dec_tree', 'clf': DecisionTreeClassifier},\n",
    "\n",
    "                  {'name': 'svm', 'clf': svm.SVC, 'clf_params': {'kernel': 'linear', 'tol': 1e-1}, 'params': {'subsamples_n': 0.002}},\n",
    "                  {'name': 'knn', 'clf': KNeighborsClassifier, 'params': {'no_random_state': True, 'subsamples_n': 0.1}},\n",
    "                  {'name': 'sgd', 'clf': SGDClassifier},\n",
    "                  {'name': 'bayes_ridge', 'clf': BayesianRidge, 'params': {'no_random_state': True}},\n",
    "                  {'name': 'elastic_net', 'clf': ElasticNet},\n",
    "                  {'name': 'log_reg', 'clf': LogisticRegression},\n",
    "                  {'name': 'perceptron', 'clf': Perceptron},\n",
    "                 ]\n",
    "\n",
    "    for e in estimators:\n",
    "        start_time = time.time()\n",
    "        clf_class = e['clf']\n",
    "        clf_params = dict(e.get('clf_params', {}))\n",
    "        params = e.get('params', {})\n",
    "        if not params.get('no_random_state'):\n",
    "            clf_params['random_state'] = RANDOM_STATE\n",
    "        if params.get('subsamples_n'):\n",
    "            subsamples_n = int(X_train.shape[0] * params['subsamples_n'])\n",
    "            x, y = X_train[:subsamples_n], y_train[:subsamples_n]\n",
    "        else:\n",
    "            x, y = X_train, y_train\n",
    "\n",
    "        clf = clf_class(**clf_params)\n",
    "        r = cross_val_score(clf, x, y,\n",
    "                            cv=2, n_jobs=4, scoring='roc_auc').mean()\n",
    "        print('{}: {:.5f} for {:.1f} sec'.format(e['name'], r, time.time() - start_time))\n",
    "        \n",
    "compare_estimators()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
